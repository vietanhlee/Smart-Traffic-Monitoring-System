{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae76a60",
   "metadata": {},
   "source": [
    "# Khai báo thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9b6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a90872",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yaml = r\"F:\\khodataset\\xe may oto\\data.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91535103",
   "metadata": {},
   "source": [
    "# Xuất các quantization model (fp16, int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eac94c",
   "metadata": {},
   "source": [
    "##  ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018b9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "WARNING half=True only compatible with GPU export, i.e. use device=0\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'onnx models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.9s, saved as 'onnx models\\best.onnx' (10.0 MB)\n",
      "\n",
      "Export complete (2.2s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\onnx models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=onnx models\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=onnx models\\best.onnx imgsz=640 data=/kaggle/input/veheicle/duyet3/data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported INT8 model: onnx models/best_int8.onnx\n"
     ]
    }
   ],
   "source": [
    "# 1. Load mô hình .pt\n",
    "model = YOLO(r\"onnx models/best.pt\")\n",
    "\n",
    "# 2. Xuất sang ONNX fp16\n",
    "\n",
    "model.export(\n",
    "    format=\"onnx\",  # Xuất ra ONNX\n",
    "    half = True,\n",
    "    dynamic=True, \n",
    "    simplify = True\n",
    ")\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# File đầu vào  FP16\n",
    "model_fp16 = r\"onnx models/best.onnx\"\n",
    "model_int8 = r\"onnx models/best_int8.onnx\"\n",
    "\n",
    "# Quantization động (Dynamic Quantization)\n",
    "quantize_dynamic(\n",
    "    model_input=model_fp16, \n",
    "    model_output=model_int8,\n",
    "    weight_type=QuantType.QUInt8  # INT8\n",
    ")\n",
    "\n",
    "print(\"✅ Exported INT8 model:\", model_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897cd40",
   "metadata": {},
   "source": [
    "## OPENVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e010b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'openvino models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2025.2.0-19140-c01cd93e24d-releases/2025/2...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m collecting INT8 calibration images from 'data=F:\\khodataset\\xe may oto\\data.yaml'\n",
      "Fast image access  (ping: 0.40.1 ms, read: 7.01.8 MB/s, size: 81.0 KB)\n",
      "\u001b[KScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115  0.0s\n",
      "WARNING:nncf:NNCF provides best results with torch==2.7.*, while current torch version is 2.8.0+cpu. If you encounter issues, consider switching to torch==2.7.*\n",
      "INFO:nncf:25 ignored nodes were found by patterns in the NNCFGraph\n",
      "INFO:nncf:1 ignored nodes were found by types in the NNCFGraph\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 181 __module.model.23.dfl/aten::view/Reshape\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 182 __module.model.23/aten::sigmoid/Sigmoid\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 198 __module.model.23.dfl/aten::transpose/Transpose\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 213 __module.model.23.dfl/aten::softmax/Softmax\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 226 __module.model.23.dfl.conv/aten::_convolution/Convolution\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 237 __module.model.23.dfl/aten::view/Reshape_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 260 __module.model.23/aten::sub/Subtract\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 261 __module.model.23/aten::add/Add_6\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 271 __module.model.23/aten::add/Add_7\n",
      "280 __module.model.23/aten::div/Divide\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 272 __module.model.23/aten::sub/Subtract_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 246 __module.model.23/aten::mul/Multiply_3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success  63.6s, saved as 'openvino models\\best_int8_openvino_model\\' (3.2 MB)\n",
      "\n",
      "Export complete (63.9s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\openvino models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=openvino models\\best_int8_openvino_model imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=openvino models\\best_int8_openvino_model imgsz=640 data=/kaggle/input/veheicle/duyet3/data.yaml int8 \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'openvino models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2025.2.0-19140-c01cd93e24d-releases/2025/2...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success  2.7s, saved as 'openvino models\\best_openvino_model\\' (5.4 MB)\n",
      "\n",
      "Export complete (3.0s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\openvino models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=openvino models\\best_openvino_model imgsz=640 half \n",
      "Validate:        yolo val task=detect model=openvino models\\best_openvino_model imgsz=640 data=/kaggle/input/veheicle/duyet3/data.yaml half \n",
      "Visualize:       https://netron.app\n",
      "✅ Xuất model OpenVINO thành công!\n"
     ]
    }
   ],
   "source": [
    "# 1. Load mô hình .pt\n",
    "model = YOLO(r\"openvino models/best.pt\")\n",
    "model.export(format=\"openvino\", dynamic=True, optimize=True, int8 = True, data = data_yaml)\n",
    "model.export(format=\"openvino\", dynamic=True, optimize=True, half = True, data = data_yaml)\n",
    "print(\"✅ Xuất model OpenVINO thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5d495",
   "metadata": {},
   "source": [
    "## MNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e39bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "WARNING INT8 export requires a missing 'data' arg for calibration. Using default 'data=coco8.yaml'.\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'mnn models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.9s, saved as 'mnn models\\best.onnx' (10.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m starting export with MNN 3.2.3...\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m export success  4.5s, saved as 'mnn models\\best.mnn' (2.7 MB)\n",
      "\n",
      "Export complete (4.7s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\mnn models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=mnn models\\best.mnn imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=mnn models\\best.mnn imgsz=640 data=/kaggle/input/veheicle/duyet3/data.yaml int8 \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'mnn models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.1s, saved as 'mnn models\\best.onnx' (10.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m starting export with MNN 3.2.3...\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m export success  1.3s, saved as 'mnn models\\best.mnn' (5.0 MB)\n",
      "\n",
      "Export complete (1.7s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\mnn models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=mnn models\\best.mnn imgsz=640 half \n",
      "Validate:        yolo val task=detect model=mnn models\\best.mnn imgsz=640 data=/kaggle/input/veheicle/duyet3/data.yaml half \n",
      "Visualize:       https://netron.app\n",
      "✅ Xuất model MNN thành công!\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"mnn models/best.pt\")\n",
    "model.export(format=\"mnn\", int8=True)  # creates 'yolo11n.mnn' with int8 weight\n",
    "import os\n",
    "os.rename(\"mnn models/best.mnn\", \"mnn models/best_int8.mnn\")\n",
    "\n",
    "model.export(format=\"mnn\", half=True)  # creates 'yolo11n.mnn' with fp16 weight\n",
    "print(\"✅ Xuất model MNN thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66cd40",
   "metadata": {},
   "source": [
    "# VALIDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87664d09",
   "metadata": {},
   "source": [
    "## MODEL GỐC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd22a4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m results = model.val(data=data_yaml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:83\u001b[39m, in \u001b[36mYOLO.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m = new_instance.\u001b[34m__dict__\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRTDETR\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model[-\u001b[32m1\u001b[39m]._get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\engine\\model.py:153\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m._new(model, task=task, verbose=verbose)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\engine\\model.py:297\u001b[39m, in \u001b[36mModel._load\u001b[39m\u001b[34m(self, weights, task)\u001b[39m\n\u001b[32m    294\u001b[39m weights = checks.check_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights).rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.ckpt = \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m.task = \u001b[38;5;28mself\u001b[39m.model.task\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mself\u001b[39m.overrides = \u001b[38;5;28mself\u001b[39m.model.args = \u001b[38;5;28mself\u001b[39m._reset_ckpt_args(\u001b[38;5;28mself\u001b[39m.model.args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1549\u001b[39m, in \u001b[36mattempt_load_one_weight\u001b[39m\u001b[34m(weight, device, inplace, fuse)\u001b[39m\n\u001b[32m   1535\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mattempt_load_one_weight\u001b[39m(weight, device=\u001b[38;5;28;01mNone\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m, fuse=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1536\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[33;03m    Load a single model weights.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1547\u001b[39m \u001b[33;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[32m   1548\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m     ckpt, weight = \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1550\u001b[39m     args = {**DEFAULT_CFG_DICT, **(ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[32m   1551\u001b[39m     model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1447\u001b[39m, in \u001b[36mtorch_safe_load\u001b[39m\u001b[34m(weight, safe_only)\u001b[39m\n\u001b[32m   1445\u001b[39m                 ckpt = torch_load(f, pickle_module=safe_pickle)\n\u001b[32m   1446\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m             ckpt = \u001b[43mtorch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.name == \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\utils\\patches.py:120\u001b[39m, in \u001b[36mtorch_load\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    118\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'best.pt'"
     ]
    }
   ],
   "source": [
    "model = YOLO(r\"orginal model/best.pt\")\n",
    "results = model.val(data=data_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18246594",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2965bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading onnx models/best.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CPUExecutionProvider\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 559.3147.9 MB/s, size: 48.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may o to\\duyet3\\valid\\labels.cache... 195 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 195/195 194550.3it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 1.0it/s 12.5s1.0ss\n",
      "                   all        195       2122      0.868      0.854      0.919      0.555\n",
      "                   car        181       1102      0.881       0.93      0.957      0.638\n",
      "                 Motor        128       1020      0.855      0.778      0.881      0.472\n",
      "Speed: 1.0ms preprocess, 54.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('onnx models/best.onnx')\n",
    "results = model.val(data=data_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading onnx models/best_int8.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CPUExecutionProvider\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 596.2193.0 MB/s, size: 54.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may o to\\duyet3\\valid\\labels.cache... 195 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 195/195  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 0.62it/s 20.9s1.7s\n",
      "                   all        195       2122      0.864      0.839      0.915       0.55\n",
      "                   car        181       1102      0.874      0.924      0.955      0.635\n",
      "                 Motor        128       1020      0.854      0.755      0.874      0.466\n",
      "Speed: 1.0ms preprocess, 94.3ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val21\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('onnx models/best_int8.onnx')\n",
    "results = model.val(data=data_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b6480",
   "metadata": {},
   "source": [
    "## OPENVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ae7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading openvino models/best_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on (CPU)...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 455.8171.9 MB/s, size: 46.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may o to\\duyet3\\valid\\labels.cache... 195 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 195/195 127855.1it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 1.4it/s 9.2s0.4ss\n",
      "                   all        195       2122      0.871      0.851       0.92      0.555\n",
      "                   car        181       1102      0.885      0.925      0.957      0.638\n",
      "                 Motor        128       1020      0.857      0.776      0.882      0.472\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val22\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('openvino models/best_openvino_model')\n",
    "results = model.val(data=data_yaml)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading openvino models/best_int8_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on (CPU)...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 516.8178.6 MB/s, size: 50.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may o to\\duyet3\\valid\\labels.cache... 195 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 195/195 198903.0it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 13/13 1.9it/s 6.8s0.5ss\n",
      "                   all        195       2122      0.863       0.85      0.916      0.552\n",
      "                   car        181       1102       0.88       0.92      0.953      0.633\n",
      "                 Motor        128       1020      0.845       0.78       0.88      0.471\n",
      "Speed: 1.1ms preprocess, 25.2ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val23\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('openvino models/best_int8_openvino_model')\n",
    "results = model.val(data=data_yaml)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3c023",
   "metadata": {},
   "source": [
    "# MNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading mnn models/best.mnn for MNN inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 567.0179.1 MB/s, size: 54.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may o to\\duyet3\\valid\\labels.cache... 195 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 195/195 195620.5it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 195/195 16.2it/s 12.0s0.1s\n",
      "                   all        195       2122      0.859      0.852      0.915      0.554\n",
      "                   car        181       1102      0.871       0.93      0.954      0.637\n",
      "                 Motor        128       1020      0.848      0.775      0.876      0.471\n",
      "Speed: 0.8ms preprocess, 51.9ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val24\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('mnn models/best.mnn')\n",
    "results = model.val(data=data_yaml)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading mnn models/best_int8.mnn for MNN inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 559.8198.7 MB/s, size: 57.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may o to\\duyet3\\valid\\labels.cache... 195 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 195/195 195573.7it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 195/195 15.4it/s 12.6s0.1s\n",
      "                   all        195       2122      0.863      0.849      0.917      0.553\n",
      "                   car        181       1102       0.87      0.929      0.954      0.636\n",
      "                 Motor        128       1020      0.856      0.769      0.879      0.471\n",
      "Speed: 0.8ms preprocess, 54.2ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val25\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('mnn models/best_int8.mnn')\n",
    "results = model.val(data=data_yaml)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
