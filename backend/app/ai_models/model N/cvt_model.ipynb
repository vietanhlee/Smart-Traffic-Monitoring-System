{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae76a60",
   "metadata": {},
   "source": [
    "# Khai báo thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9b6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a90872",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yaml = r\"F:\\khodataset\\xe may oto\\data.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91535103",
   "metadata": {},
   "source": [
    "# Xuất các quantization model (fp16, int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eac94c",
   "metadata": {},
   "source": [
    "##  ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018b9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "WARNING half=True only compatible with GPU export, i.e. use device=0\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'onnx models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  3.7s, saved as 'onnx models\\best.onnx' (10.1 MB)\n",
      "\n",
      "Export complete (4.0s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\onnx models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=onnx models\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=onnx models\\best.onnx imgsz=640 data=/kaggle/input/veheicle/xe may oto/data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported INT8 model: onnx models/best_int8.onnx\n"
     ]
    }
   ],
   "source": [
    "# 1. Load mô hình .pt\n",
    "model = YOLO(r\"onnx models/best.pt\")\n",
    "\n",
    "# 2. Xuất sang ONNX fp16\n",
    "\n",
    "model.export(\n",
    "    format=\"onnx\",  # Xuất ra ONNX\n",
    "    half = True,\n",
    "    dynamic=True, \n",
    "    simplify = True\n",
    ")\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# File đầu vào  FP16\n",
    "model_fp16 = r\"onnx models/best.onnx\"\n",
    "model_int8 = r\"onnx models/best_int8.onnx\"\n",
    "\n",
    "# Quantization động (Dynamic Quantization)\n",
    "quantize_dynamic(\n",
    "    model_input=model_fp16, \n",
    "    model_output=model_int8,\n",
    "    weight_type=QuantType.QUInt8  # INT8\n",
    ")\n",
    "\n",
    "print(\"✅ Exported INT8 model:\", model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.169  Python-3.13.5 torch-2.7.1+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      " ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'torchscript models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.7.1+cpu...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m optimizing for mobile...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success  2.2s, saved as 'torchscript models\\best.torchscript' (10.0 MB)\n",
      "\n",
      "Export complete (2.7s)\n",
      "Results saved to \u001b[1mG:\\nhap\\app\\AI models\\model N\\torchscript models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=torchscript models\\best.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=torchscript models\\best.torchscript imgsz=640 data=/kaggle/input/veheicle/xe may oto/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "✅ Xuất model TorchScript thành công!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = YOLO(r\"torchscript models/best.pt\")\n",
    "model.export(format=\"torchscript\", device=\"cpu\")\n",
    "print(\"✅ Xuất model TorchScript thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897cd40",
   "metadata": {},
   "source": [
    "## OPENVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e010b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'openvino models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2025.2.0-19140-c01cd93e24d-releases/2025/2...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m collecting INT8 calibration images from 'data=F:\\khodataset\\xe may oto\\data.yaml'\n",
      "Fast image access  (ping: 0.20.0 ms, read: 16.04.1 MB/s, size: 85.6 KB)\n",
      "\u001b[KScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115  0.0s\n",
      "WARNING:nncf:NNCF provides best results with torch==2.7.*, while current torch version is 2.8.0+cpu. If you encounter issues, consider switching to torch==2.7.*\n",
      "INFO:nncf:25 ignored nodes were found by patterns in the NNCFGraph\n",
      "INFO:nncf:1 ignored nodes were found by types in the NNCFGraph\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 181 __module.model.23.dfl/aten::view/Reshape\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 182 __module.model.23/aten::sigmoid/Sigmoid\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 198 __module.model.23.dfl/aten::transpose/Transpose\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 213 __module.model.23.dfl/aten::softmax/Softmax\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 226 __module.model.23.dfl.conv/aten::_convolution/Convolution\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 237 __module.model.23.dfl/aten::view/Reshape_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 260 __module.model.23/aten::sub/Subtract\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 261 __module.model.23/aten::add/Add_6\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 271 __module.model.23/aten::add/Add_7\n",
      "280 __module.model.23/aten::div/Divide\n",
      "\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 272 __module.model.23/aten::sub/Subtract_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 246 __module.model.23/aten::mul/Multiply_3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success  72.8s, saved as 'openvino models\\best_int8_openvino_model\\' (3.2 MB)\n",
      "\n",
      "Export complete (73.1s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\openvino models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=openvino models\\best_int8_openvino_model imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=openvino models\\best_int8_openvino_model imgsz=640 data=/kaggle/input/veheicle/xe may oto/data.yaml int8 \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'openvino models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2025.2.0-19140-c01cd93e24d-releases/2025/2...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success  3.9s, saved as 'openvino models\\best_openvino_model\\' (5.4 MB)\n",
      "\n",
      "Export complete (4.3s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\openvino models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=openvino models\\best_openvino_model imgsz=640 half \n",
      "Validate:        yolo val task=detect model=openvino models\\best_openvino_model imgsz=640 data=/kaggle/input/veheicle/xe may oto/data.yaml half \n",
      "Visualize:       https://netron.app\n",
      "✅ Xuất model OpenVINO thành công!\n"
     ]
    }
   ],
   "source": [
    "# 1. Load mô hình .pt\n",
    "model = YOLO(r\"openvino models/best.pt\")\n",
    "model.export(format=\"openvino\", dynamic=True, optimize=True, int8 = True, data = data_yaml)\n",
    "model.export(format=\"openvino\", dynamic=True, optimize=True, half = True, data = data_yaml)\n",
    "print(\"✅ Xuất model OpenVINO thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5d495",
   "metadata": {},
   "source": [
    "## MNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e39bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "WARNING INT8 export requires a missing 'data' arg for calibration. Using default 'data=coco8.yaml'.\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'mnn models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.0s, saved as 'mnn models\\best.onnx' (10.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m starting export with MNN 3.2.3...\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m export success  2.7s, saved as 'mnn models\\best.mnn' (2.7 MB)\n",
      "\n",
      "Export complete (3.0s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\mnn models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=mnn models\\best.mnn imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=mnn models\\best.mnn imgsz=640 data=/kaggle/input/veheicle/xe may oto/data.yaml int8 \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'mnn models\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (5.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.9s, saved as 'mnn models\\best.onnx' (10.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m starting export with MNN 3.2.3...\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m export success  1.1s, saved as 'mnn models\\best.mnn' (5.0 MB)\n",
      "\n",
      "Export complete (1.4s)\n",
      "Results saved to \u001b[1mG:\\smart-transportation-system\\app\\AI models\\model N\\mnn models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=mnn models\\best.mnn imgsz=640 half \n",
      "Validate:        yolo val task=detect model=mnn models\\best.mnn imgsz=640 data=/kaggle/input/veheicle/xe may oto/data.yaml half \n",
      "Visualize:       https://netron.app\n",
      "✅ Xuất model MNN thành công!\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"mnn models/best.pt\")\n",
    "model.export(format=\"mnn\", int8=True)  # creates 'yolo11n.mnn' with int8 weight\n",
    "import os\n",
    "os.rename(\"mnn models/best.mnn\", \"mnn models/best_int8.mnn\")\n",
    "\n",
    "model.export(format=\"mnn\", half=True)  # creates 'yolo11n.mnn' with fp16 weight\n",
    "print(\"✅ Xuất model MNN thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66cd40",
   "metadata": {},
   "source": [
    "# VALIDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87664d09",
   "metadata": {},
   "source": [
    "## MODEL GỐC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd22a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 610.5246.9 MB/s, size: 84.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115 1098320.6it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 70/70 0.47it/s 2:281.2ss\n",
      "                   all       1115      10014       0.89      0.904      0.942      0.638\n",
      "                   car       1096       5940      0.902      0.942       0.96      0.711\n",
      "                 Motor        915       4074      0.877      0.867      0.923      0.564\n",
      "Speed: 1.3ms preprocess, 111.7ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(r\"original model/best.pt\")\n",
    "results = model.val(data=data_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18246594",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2965bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading onnx models/best.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CPUExecutionProvider\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 155.9323.3 MB/s, size: 88.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 70/70 0.81it/s 1:261.2sss\n",
      "                   all       1115      10014       0.89      0.904      0.942      0.638\n",
      "                   car       1096       5940      0.902      0.942       0.96      0.711\n",
      "                 Motor        915       4074      0.877      0.867      0.923      0.564\n",
      "Speed: 0.9ms preprocess, 61.4ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('onnx models/best.onnx')\n",
    "results = model.val(data=data_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f83275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading onnx models/best_int8.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CPUExecutionProvider\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 323.5110.7 MB/s, size: 78.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115 1061427.4it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 70/70 0.44it/s 2:382.6ss\n",
      "                   all       1115      10014      0.885      0.903       0.94      0.631\n",
      "                   car       1096       5940      0.902      0.937       0.96      0.706\n",
      "                 Motor        915       4074      0.868      0.869       0.92      0.556\n",
      "Speed: 1.0ms preprocess, 128.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('onnx models/best_int8.onnx')\n",
    "results = model.val(data=data_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b6480",
   "metadata": {},
   "source": [
    "## OPENVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5ae7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading openvino models/best_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on (CPU)...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 97.023.3 MB/s, size: 82.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115 991235.5it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 70/70 2.0it/s 35.6s0.4ss\n",
      "                   all       1115      10014      0.891      0.904      0.942      0.637\n",
      "                   car       1096       5940      0.903      0.942      0.961       0.71\n",
      "                 Motor        915       4074      0.878      0.866      0.923      0.563\n",
      "Speed: 1.1ms preprocess, 19.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('openvino models/best_openvino_model')\n",
    "results = model.val(data=data_yaml)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4600df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading openvino models/best_int8_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on (CPU)...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 479.2183.9 MB/s, size: 85.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115 1114018.3it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 70/70 2.0it/s 35.1s0.5ss\n",
      "                   all       1115      10014      0.892      0.902      0.941      0.635\n",
      "                   car       1096       5940      0.905      0.939       0.96       0.71\n",
      "                 Motor        915       4074      0.879      0.865      0.921       0.56\n",
      "Speed: 0.9ms preprocess, 21.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('openvino models/best_int8_openvino_model')\n",
    "results = model.val(data=data_yaml)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3c023",
   "metadata": {},
   "source": [
    "# MNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0c22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading mnn models/best.mnn for MNN inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 497.2199.7 MB/s, size: 85.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115 2126716.2it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1115/1115 14.9it/s 1:15<0.1s\n",
      "                   all       1115      10014      0.894        0.9       0.94      0.634\n",
      "                   car       1096       5940      0.898      0.937      0.956      0.708\n",
      "                 Motor        915       4074       0.89      0.863      0.923      0.561\n",
      "Speed: 0.8ms preprocess, 56.3ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('mnn models/best.mnn')\n",
    "results = model.val(data=data_yaml)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c6bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12600H)\n",
      "Loading mnn models/best_int8.mnn for MNN inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 445.8127.6 MB/s, size: 94.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\khodataset\\xe may oto\\valid\\labels.cache... 1115 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1115/1115 1821125.0it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1115/1115 9.3it/s 1:599<0.1s\n",
      "                   all       1115      10014      0.893      0.898      0.939      0.634\n",
      "                   car       1096       5940      0.897      0.937      0.956      0.708\n",
      "                 Motor        915       4074      0.889      0.859      0.922      0.561\n",
      "Speed: 0.8ms preprocess, 96.2ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('mnn models/best_int8.mnn')\n",
    "results = model.val(data=data_yaml)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a70f3f",
   "metadata": {},
   "source": [
    "## TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf373e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.191  Python-3.12.11 torch-2.8.0+cpu \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33m./TensorRT/best.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mengine\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\engine\\model.py:736\u001b[39m, in \u001b[36mModel.export\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    728\u001b[39m custom = {\n\u001b[32m    729\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimgsz\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model.args[\u001b[33m\"\u001b[39m\u001b[33mimgsz\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    730\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    734\u001b[39m }  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[32m    735\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mexport\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\engine\\exporter.py:310\u001b[39m, in \u001b[36mExporter.__call__\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    308\u001b[39m     LOGGER.warning(\u001b[33m\"\u001b[39m\u001b[33mExporting on CPU while CUDA is available, setting device=0 for faster export on GPU.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    309\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.device = \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# update device to \"0\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[38;5;28mself\u001b[39m.device = \u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# Argument compatibility checks\u001b[39;00m\n\u001b[32m    313\u001b[39m fmt_keys = fmts_dict[\u001b[33m\"\u001b[39m\u001b[33mArguments\u001b[39m\u001b[33m\"\u001b[39m][flags.index(\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\levie\\anaconda3\\envs\\py312\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:204\u001b[39m, in \u001b[36mselect_device\u001b[39m\u001b[34m(device, batch, newline, verbose)\u001b[39m\n\u001b[32m    197\u001b[39m         LOGGER.info(s)\n\u001b[32m    198\u001b[39m         install = (\n\u001b[32m    199\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCUDA devices are seen by torch.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.device_count() == \u001b[32m0\u001b[39m\n\u001b[32m    202\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    205\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid CUDA \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdevice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m requested.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Use \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdevice=cpu\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or pass valid CUDA device(s) if available,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m i.e. \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdevice=0\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdevice=0,1,2,3\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for Multi-GPU.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mtorch.cuda.is_available(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.is_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mtorch.cuda.device_count(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.device_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mos.environ[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisible\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    212\u001b[39m         )\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch.cuda.is_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n\u001b[32m    215\u001b[39m     devices = device.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# i.e. \"0,1\" -> [\"0\", \"1\"]\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('./TensorRT/best.pt')\n",
    "model.export(format = 'engine', int8 = True, dynamic = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
